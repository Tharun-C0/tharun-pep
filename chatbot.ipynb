{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH3uOeazwDWa1jz6F99CnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharun-C0/tharun-pep/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LxqDnjh5L3lV"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch accelerate gradio sympy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import gradio as gr\n",
        "import sympy as sp\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"
      ],
      "metadata": {
        "id": "-ivcOjwT3p3c"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3k8bCcq3sJc",
        "outputId": "fce7b5d0-b31a-490d-dce8-49ef8dfdc843"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 1024)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 1024)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 16)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-23): 23 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 1024)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 16)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-23): 23 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
              "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_simple_math(expr):\n",
        "    return bool(re.fullmatch(r\"[0-9+\\-*/(). ]+\", expr))\n"
      ],
      "metadata": {
        "id": "Xwg_62UD3tZf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_math_exact(expr):\n",
        "    try:\n",
        "        result = sp.sympify(expr, evaluate=True)\n",
        "        return result\n",
        "    except:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "v1rsfUbz3xDT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_math(expr, result):\n",
        "    prompt = f\"\"\"\n",
        "    Explain how to solve the following math problem step by step.\n",
        "    Do NOT calculate the final answer.\n",
        "    The final answer is already provided.\n",
        "\n",
        "    Problem: {expr}\n",
        "    Final Answer: {result}\n",
        "\n",
        "    Explanation:\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=160,\n",
        "        temperature=0.2,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "hB0n0qOa3yMs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_definition_question(text):\n",
        "    keywords = [\n",
        "        \"define\", \"definition\", \"what is\", \"explain\",\n",
        "        \"meaning of\", \"describe\", \"tell me about\"\n",
        "    ]\n",
        "    return any(k in text.lower() for k in keywords)\n"
      ],
      "metadata": {
        "id": "WP5RbuEg3zfN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = []\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "\n",
        "\n",
        "    if is_simple_math(user_input):\n",
        "        exact_result = solve_math_exact(user_input)\n",
        "\n",
        "        if exact_result is not None:\n",
        "            explanation = explain_math(user_input, exact_result)\n",
        "\n",
        "            return (\n",
        "                f\"ðŸ§® **Math Solution**\\n\\n\"\n",
        "                f\"Expression: {user_input}\\n\"\n",
        "                f\"Final Answer: {exact_result}\\n\\n\"\n",
        "                f\"Explanation:\\n{explanation}\"\n",
        "            )\n",
        "\n",
        "\n",
        "    if is_definition_question(user_input):\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert computer science professor.\n",
        "        Give a clear, accurate, and well-structured explanation.\n",
        "        Use simple language and examples if needed.\n",
        "\n",
        "        Question: {user_input}\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "    else:\n",
        "\n",
        "        context = \" \".join(conversation_history[-4:])\n",
        "        prompt = f\"\"\"\n",
        "        You are a friendly and intelligent AI assistant.\n",
        "        Maintain a natural conversation.\n",
        "\n",
        "        Conversation history:\n",
        "        {context}\n",
        "\n",
        "        User: {user_input}\n",
        "        Assistant:\n",
        "        \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.3,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "    conversation_history.append(f\"Assistant: {response}\")\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "OF91GyVd31G8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_ui(user_message, history):\n",
        "    bot_reply = chatbot_response(user_message)\n",
        "    history.append((user_message, bot_reply))\n",
        "    return history, \"\"\n"
      ],
      "metadata": {
        "id": "tBv5t_zx328Q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"THARUN  Chatbot\")\n",
        "    gr.Markdown(\"Handles conversation, definitions, and **accurate step-by-step math reasoning**\")\n",
        "\n",
        "    chatbot = gr.Chatbot(height=450)\n",
        "    msg = gr.Textbox(placeholder=\"Ask anything (math, concepts, chat)...\")\n",
        "    clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    msg.submit(chat_ui, [msg, chatbot], [chatbot, msg])\n",
        "    clear.click(lambda: [], None, chatbot)\n",
        "\n",
        "demo.launch(share=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "bzbGZ8ff35RE",
        "outputId": "f6e056b9-e8f9-4c49-a38d-ff5f7ec9e0cc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3845213473.py:5: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=450)\n",
            "/tmp/ipython-input-3845213473.py:5: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(height=450)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7862, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}